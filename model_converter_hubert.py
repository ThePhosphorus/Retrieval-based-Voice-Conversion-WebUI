import torch

model = torch.load('assets/hubert/hubert_base.pt')

weights = model['model']

new_weights = {}

conversion_weight_labels = {
#'mask_emb':'',
#'label_embs_concat':'',

'feature_extractor.conv_layers.0.0.weight':'feature_extractor.conv_layers.0.conv.weight',
'feature_extractor.conv_layers.0.2.weight':'feature_extractor.conv_layers.0.layer_norm.weight',
'feature_extractor.conv_layers.0.2.bias':'feature_extractor.conv_layers.0.layer_norm.bias',
'feature_extractor.conv_layers.1.0.weight':'feature_extractor.conv_layers.1.conv.weight',
'feature_extractor.conv_layers.2.0.weight':'feature_extractor.conv_layers.2.conv.weight',
'feature_extractor.conv_layers.3.0.weight':'feature_extractor.conv_layers.3.conv.weight',
'feature_extractor.conv_layers.4.0.weight':'feature_extractor.conv_layers.4.conv.weight',
'feature_extractor.conv_layers.5.0.weight':'feature_extractor.conv_layers.5.conv.weight',
'feature_extractor.conv_layers.6.0.weight':'feature_extractor.conv_layers.6.conv.weight',

'post_extract_proj.weight':'encoder.feature_projection.projection.weight',
'post_extract_proj.bias':'encoder.feature_projection.projection.bias',

'encoder.pos_conv.0.bias':'encoder.transformer.pos_conv_embed.conv.bias',
'encoder.pos_conv.0.weight_g':'encoder.transformer.pos_conv_embed.conv.parametrizations.weight.original0', # ?
'encoder.pos_conv.0.weight_v':'encoder.transformer.pos_conv_embed.conv.parametrizations.weight.original1', #?
'encoder.layers.0.self_attn.k_proj.weight':'encoder.transformer.layers.0.attention.k_proj.weight',
'encoder.layers.0.self_attn.k_proj.bias':'encoder.transformer.layers.0.attention.k_proj.bias',
'encoder.layers.0.self_attn.v_proj.weight':'encoder.transformer.layers.0.attention.v_proj.weight',
'encoder.layers.0.self_attn.v_proj.bias':'encoder.transformer.layers.0.attention.v_proj.bias',
'encoder.layers.0.self_attn.q_proj.weight':'encoder.transformer.layers.0.attention.q_proj.weight',
'encoder.layers.0.self_attn.q_proj.bias':'encoder.transformer.layers.0.attention.q_proj.bias',
'encoder.layers.0.self_attn.out_proj.weight':'encoder.transformer.layers.0.attention.out_proj.weight',
'encoder.layers.0.self_attn.out_proj.bias':'encoder.transformer.layers.0.attention.out_proj.bias',
'encoder.layers.0.self_attn_layer_norm.weight':'encoder.transformer.layers.0.layer_norm.weight',
'encoder.layers.0.self_attn_layer_norm.bias':'encoder.transformer.layers.0.layer_norm.bias',
'encoder.layers.0.fc1.weight':'encoder.transformer.layers.0.feed_forward.intermediate_dense.weight',
'encoder.layers.0.fc1.bias':'encoder.transformer.layers.0.feed_forward.intermediate_dense.bias',
'encoder.layers.0.fc2.weight':'encoder.transformer.layers.0.feed_forward.output_dense.weight',
'encoder.layers.0.fc2.bias':'encoder.transformer.layers.0.feed_forward.output_dense.bias',
'encoder.layers.0.final_layer_norm.weight':'encoder.transformer.layers.0.final_layer_norm.weight',
'encoder.layers.0.final_layer_norm.bias':'encoder.transformer.layers.0.final_layer_norm.bias',

'encoder.layers.1.self_attn.k_proj.weight':    'encoder.transformer.layers.1.attention.k_proj.weight',
'encoder.layers.1.self_attn.k_proj.bias':      'encoder.transformer.layers.1.attention.k_proj.bias',
'encoder.layers.1.self_attn.v_proj.weight':    'encoder.transformer.layers.1.attention.v_proj.weight',
'encoder.layers.1.self_attn.v_proj.bias':      'encoder.transformer.layers.1.attention.v_proj.bias',
'encoder.layers.1.self_attn.q_proj.weight':    'encoder.transformer.layers.1.attention.q_proj.weight',
'encoder.layers.1.self_attn.q_proj.bias':      'encoder.transformer.layers.1.attention.q_proj.bias',
'encoder.layers.1.self_attn.out_proj.weight':  'encoder.transformer.layers.1.attention.out_proj.weight',
'encoder.layers.1.self_attn.out_proj.bias':    'encoder.transformer.layers.1.attention.out_proj.bias',
'encoder.layers.1.self_attn_layer_norm.weight':'encoder.transformer.layers.1.layer_norm.weight',
'encoder.layers.1.self_attn_layer_norm.bias':  'encoder.transformer.layers.1.layer_norm.bias',
'encoder.layers.1.fc1.weight':                 'encoder.transformer.layers.1.feed_forward.intermediate_dense.weight',
'encoder.layers.1.fc1.bias':                   'encoder.transformer.layers.1.feed_forward.intermediate_dense.bias',
'encoder.layers.1.fc2.weight':                 'encoder.transformer.layers.1.feed_forward.output_dense.weight',
'encoder.layers.1.fc2.bias':                   'encoder.transformer.layers.1.feed_forward.output_dense.bias',
'encoder.layers.1.final_layer_norm.weight':    'encoder.transformer.layers.1.final_layer_norm.weight',
'encoder.layers.1.final_layer_norm.bias':      'encoder.transformer.layers.1.final_layer_norm.bias',

'encoder.layers.2.self_attn.k_proj.weight':    'encoder.transformer.layers.2.attention.k_proj.weight',
'encoder.layers.2.self_attn.k_proj.bias':      'encoder.transformer.layers.2.attention.k_proj.bias',
'encoder.layers.2.self_attn.v_proj.weight':    'encoder.transformer.layers.2.attention.v_proj.weight',
'encoder.layers.2.self_attn.v_proj.bias':      'encoder.transformer.layers.2.attention.v_proj.bias',
'encoder.layers.2.self_attn.q_proj.weight':    'encoder.transformer.layers.2.attention.q_proj.weight',
'encoder.layers.2.self_attn.q_proj.bias':      'encoder.transformer.layers.2.attention.q_proj.bias',
'encoder.layers.2.self_attn.out_proj.weight':  'encoder.transformer.layers.2.attention.out_proj.weight',
'encoder.layers.2.self_attn.out_proj.bias':    'encoder.transformer.layers.2.attention.out_proj.bias',
'encoder.layers.2.self_attn_layer_norm.weight':'encoder.transformer.layers.2.layer_norm.weight',
'encoder.layers.2.self_attn_layer_norm.bias':  'encoder.transformer.layers.2.layer_norm.bias',
'encoder.layers.2.fc1.weight':                 'encoder.transformer.layers.2.feed_forward.intermediate_dense.weight',
'encoder.layers.2.fc1.bias':                   'encoder.transformer.layers.2.feed_forward.intermediate_dense.bias',
'encoder.layers.2.fc2.weight':                 'encoder.transformer.layers.2.feed_forward.output_dense.weight',
'encoder.layers.2.fc2.bias':                   'encoder.transformer.layers.2.feed_forward.output_dense.bias',
'encoder.layers.2.final_layer_norm.weight':    'encoder.transformer.layers.2.final_layer_norm.weight',
'encoder.layers.2.final_layer_norm.bias':      'encoder.transformer.layers.2.final_layer_norm.bias',

'encoder.layers.3.self_attn.k_proj.weight':    'encoder.transformer.layers.3.attention.k_proj.weight',
'encoder.layers.3.self_attn.k_proj.bias':      'encoder.transformer.layers.3.attention.k_proj.bias',
'encoder.layers.3.self_attn.v_proj.weight':    'encoder.transformer.layers.3.attention.v_proj.weight',
'encoder.layers.3.self_attn.v_proj.bias':      'encoder.transformer.layers.3.attention.v_proj.bias',
'encoder.layers.3.self_attn.q_proj.weight':    'encoder.transformer.layers.3.attention.q_proj.weight',
'encoder.layers.3.self_attn.q_proj.bias':      'encoder.transformer.layers.3.attention.q_proj.bias',
'encoder.layers.3.self_attn.out_proj.weight':  'encoder.transformer.layers.3.attention.out_proj.weight',
'encoder.layers.3.self_attn.out_proj.bias':    'encoder.transformer.layers.3.attention.out_proj.bias',
'encoder.layers.3.self_attn_layer_norm.weight':'encoder.transformer.layers.3.layer_norm.weight',
'encoder.layers.3.self_attn_layer_norm.bias':  'encoder.transformer.layers.3.layer_norm.bias',
'encoder.layers.3.fc1.weight':                 'encoder.transformer.layers.3.feed_forward.intermediate_dense.weight',
'encoder.layers.3.fc1.bias':                   'encoder.transformer.layers.3.feed_forward.intermediate_dense.bias',
'encoder.layers.3.fc2.weight':                 'encoder.transformer.layers.3.feed_forward.output_dense.weight',
'encoder.layers.3.fc2.bias':                   'encoder.transformer.layers.3.feed_forward.output_dense.bias',
'encoder.layers.3.final_layer_norm.weight':    'encoder.transformer.layers.3.final_layer_norm.weight',
'encoder.layers.3.final_layer_norm.bias':      'encoder.transformer.layers.3.final_layer_norm.bias',

'encoder.layers.4.self_attn.k_proj.weight':    'encoder.transformer.layers.4.attention.k_proj.weight',
'encoder.layers.4.self_attn.k_proj.bias':      'encoder.transformer.layers.4.attention.k_proj.bias',
'encoder.layers.4.self_attn.v_proj.weight':    'encoder.transformer.layers.4.attention.v_proj.weight',
'encoder.layers.4.self_attn.v_proj.bias':      'encoder.transformer.layers.4.attention.v_proj.bias',
'encoder.layers.4.self_attn.q_proj.weight':    'encoder.transformer.layers.4.attention.q_proj.weight',
'encoder.layers.4.self_attn.q_proj.bias':      'encoder.transformer.layers.4.attention.q_proj.bias',
'encoder.layers.4.self_attn.out_proj.weight':  'encoder.transformer.layers.4.attention.out_proj.weight',
'encoder.layers.4.self_attn.out_proj.bias':    'encoder.transformer.layers.4.attention.out_proj.bias',
'encoder.layers.4.self_attn_layer_norm.weight':'encoder.transformer.layers.4.layer_norm.weight',
'encoder.layers.4.self_attn_layer_norm.bias':  'encoder.transformer.layers.4.layer_norm.bias',
'encoder.layers.4.fc1.weight':                 'encoder.transformer.layers.4.feed_forward.intermediate_dense.weight',
'encoder.layers.4.fc1.bias':                   'encoder.transformer.layers.4.feed_forward.intermediate_dense.bias',
'encoder.layers.4.fc2.weight':                 'encoder.transformer.layers.4.feed_forward.output_dense.weight',
'encoder.layers.4.fc2.bias':                   'encoder.transformer.layers.4.feed_forward.output_dense.bias',
'encoder.layers.4.final_layer_norm.weight':    'encoder.transformer.layers.4.final_layer_norm.weight',
'encoder.layers.4.final_layer_norm.bias':      'encoder.transformer.layers.4.final_layer_norm.bias',

'encoder.layers.5.self_attn.k_proj.weight':    'encoder.transformer.layers.5.attention.k_proj.weight',
'encoder.layers.5.self_attn.k_proj.bias':      'encoder.transformer.layers.5.attention.k_proj.bias',
'encoder.layers.5.self_attn.v_proj.weight':    'encoder.transformer.layers.5.attention.v_proj.weight',
'encoder.layers.5.self_attn.v_proj.bias':      'encoder.transformer.layers.5.attention.v_proj.bias',
'encoder.layers.5.self_attn.q_proj.weight':    'encoder.transformer.layers.5.attention.q_proj.weight',
'encoder.layers.5.self_attn.q_proj.bias':      'encoder.transformer.layers.5.attention.q_proj.bias',
'encoder.layers.5.self_attn.out_proj.weight':  'encoder.transformer.layers.5.attention.out_proj.weight',
'encoder.layers.5.self_attn.out_proj.bias':    'encoder.transformer.layers.5.attention.out_proj.bias',
'encoder.layers.5.self_attn_layer_norm.weight':'encoder.transformer.layers.5.layer_norm.weight',
'encoder.layers.5.self_attn_layer_norm.bias':  'encoder.transformer.layers.5.layer_norm.bias',
'encoder.layers.5.fc1.weight':                 'encoder.transformer.layers.5.feed_forward.intermediate_dense.weight',
'encoder.layers.5.fc1.bias':                   'encoder.transformer.layers.5.feed_forward.intermediate_dense.bias',
'encoder.layers.5.fc2.weight':                 'encoder.transformer.layers.5.feed_forward.output_dense.weight',
'encoder.layers.5.fc2.bias':                   'encoder.transformer.layers.5.feed_forward.output_dense.bias',
'encoder.layers.5.final_layer_norm.weight':    'encoder.transformer.layers.5.final_layer_norm.weight',
'encoder.layers.5.final_layer_norm.bias':      'encoder.transformer.layers.5.final_layer_norm.bias',

'encoder.layers.6.self_attn.k_proj.weight':    'encoder.transformer.layers.6.attention.k_proj.weight',
'encoder.layers.6.self_attn.k_proj.bias':      'encoder.transformer.layers.6.attention.k_proj.bias',
'encoder.layers.6.self_attn.v_proj.weight':    'encoder.transformer.layers.6.attention.v_proj.weight',
'encoder.layers.6.self_attn.v_proj.bias':      'encoder.transformer.layers.6.attention.v_proj.bias',
'encoder.layers.6.self_attn.q_proj.weight':    'encoder.transformer.layers.6.attention.q_proj.weight',
'encoder.layers.6.self_attn.q_proj.bias':      'encoder.transformer.layers.6.attention.q_proj.bias',
'encoder.layers.6.self_attn.out_proj.weight':  'encoder.transformer.layers.6.attention.out_proj.weight',
'encoder.layers.6.self_attn.out_proj.bias':    'encoder.transformer.layers.6.attention.out_proj.bias',
'encoder.layers.6.self_attn_layer_norm.weight':'encoder.transformer.layers.6.layer_norm.weight',
'encoder.layers.6.self_attn_layer_norm.bias':  'encoder.transformer.layers.6.layer_norm.bias',
'encoder.layers.6.fc1.weight':                 'encoder.transformer.layers.6.feed_forward.intermediate_dense.weight',
'encoder.layers.6.fc1.bias':                   'encoder.transformer.layers.6.feed_forward.intermediate_dense.bias',
'encoder.layers.6.fc2.weight':                 'encoder.transformer.layers.6.feed_forward.output_dense.weight',
'encoder.layers.6.fc2.bias':                   'encoder.transformer.layers.6.feed_forward.output_dense.bias',
'encoder.layers.6.final_layer_norm.weight':    'encoder.transformer.layers.6.final_layer_norm.weight',
'encoder.layers.6.final_layer_norm.bias':      'encoder.transformer.layers.6.final_layer_norm.bias',

'encoder.layers.7.self_attn.k_proj.weight':    'encoder.transformer.layers.7.attention.k_proj.weight',
'encoder.layers.7.self_attn.k_proj.bias':      'encoder.transformer.layers.7.attention.k_proj.bias',
'encoder.layers.7.self_attn.v_proj.weight':    'encoder.transformer.layers.7.attention.v_proj.weight',
'encoder.layers.7.self_attn.v_proj.bias':      'encoder.transformer.layers.7.attention.v_proj.bias',
'encoder.layers.7.self_attn.q_proj.weight':    'encoder.transformer.layers.7.attention.q_proj.weight',
'encoder.layers.7.self_attn.q_proj.bias':      'encoder.transformer.layers.7.attention.q_proj.bias',
'encoder.layers.7.self_attn.out_proj.weight':  'encoder.transformer.layers.7.attention.out_proj.weight',
'encoder.layers.7.self_attn.out_proj.bias':    'encoder.transformer.layers.7.attention.out_proj.bias',
'encoder.layers.7.self_attn_layer_norm.weight':'encoder.transformer.layers.7.layer_norm.weight',
'encoder.layers.7.self_attn_layer_norm.bias':  'encoder.transformer.layers.7.layer_norm.bias',
'encoder.layers.7.fc1.weight':                 'encoder.transformer.layers.7.feed_forward.intermediate_dense.weight',
'encoder.layers.7.fc1.bias':                   'encoder.transformer.layers.7.feed_forward.intermediate_dense.bias',
'encoder.layers.7.fc2.weight':                 'encoder.transformer.layers.7.feed_forward.output_dense.weight',
'encoder.layers.7.fc2.bias':                   'encoder.transformer.layers.7.feed_forward.output_dense.bias',
'encoder.layers.7.final_layer_norm.weight':    'encoder.transformer.layers.7.final_layer_norm.weight',
'encoder.layers.7.final_layer_norm.bias':      'encoder.transformer.layers.7.final_layer_norm.bias',

'encoder.layers.8.self_attn.k_proj.weight':    'encoder.transformer.layers.8.attention.k_proj.weight',
'encoder.layers.8.self_attn.k_proj.bias':      'encoder.transformer.layers.8.attention.k_proj.bias',
'encoder.layers.8.self_attn.v_proj.weight':    'encoder.transformer.layers.8.attention.v_proj.weight',
'encoder.layers.8.self_attn.v_proj.bias':      'encoder.transformer.layers.8.attention.v_proj.bias',
'encoder.layers.8.self_attn.q_proj.weight':    'encoder.transformer.layers.8.attention.q_proj.weight',
'encoder.layers.8.self_attn.q_proj.bias':      'encoder.transformer.layers.8.attention.q_proj.bias',
'encoder.layers.8.self_attn.out_proj.weight':  'encoder.transformer.layers.8.attention.out_proj.weight',
'encoder.layers.8.self_attn.out_proj.bias':    'encoder.transformer.layers.8.attention.out_proj.bias',
'encoder.layers.8.self_attn_layer_norm.weight':'encoder.transformer.layers.8.layer_norm.weight',
'encoder.layers.8.self_attn_layer_norm.bias':  'encoder.transformer.layers.8.layer_norm.bias',
'encoder.layers.8.fc1.weight':                 'encoder.transformer.layers.8.feed_forward.intermediate_dense.weight',
'encoder.layers.8.fc1.bias':                   'encoder.transformer.layers.8.feed_forward.intermediate_dense.bias',
'encoder.layers.8.fc2.weight':                 'encoder.transformer.layers.8.feed_forward.output_dense.weight',
'encoder.layers.8.fc2.bias':                   'encoder.transformer.layers.8.feed_forward.output_dense.bias',
'encoder.layers.8.final_layer_norm.weight':    'encoder.transformer.layers.8.final_layer_norm.weight',
'encoder.layers.8.final_layer_norm.bias':      'encoder.transformer.layers.8.final_layer_norm.bias',

'encoder.layers.9.self_attn.k_proj.weight':    'encoder.transformer.layers.9.attention.k_proj.weight',
'encoder.layers.9.self_attn.k_proj.bias':      'encoder.transformer.layers.9.attention.k_proj.bias',
'encoder.layers.9.self_attn.v_proj.weight':    'encoder.transformer.layers.9.attention.v_proj.weight',
'encoder.layers.9.self_attn.v_proj.bias':      'encoder.transformer.layers.9.attention.v_proj.bias',
'encoder.layers.9.self_attn.q_proj.weight':    'encoder.transformer.layers.9.attention.q_proj.weight',
'encoder.layers.9.self_attn.q_proj.bias':      'encoder.transformer.layers.9.attention.q_proj.bias',
'encoder.layers.9.self_attn.out_proj.weight':  'encoder.transformer.layers.9.attention.out_proj.weight',
'encoder.layers.9.self_attn.out_proj.bias':    'encoder.transformer.layers.9.attention.out_proj.bias',
'encoder.layers.9.self_attn_layer_norm.weight':'encoder.transformer.layers.9.layer_norm.weight',
'encoder.layers.9.self_attn_layer_norm.bias':  'encoder.transformer.layers.9.layer_norm.bias',
'encoder.layers.9.fc1.weight':                 'encoder.transformer.layers.9.feed_forward.intermediate_dense.weight',
'encoder.layers.9.fc1.bias':                   'encoder.transformer.layers.9.feed_forward.intermediate_dense.bias',
'encoder.layers.9.fc2.weight':                 'encoder.transformer.layers.9.feed_forward.output_dense.weight',
'encoder.layers.9.fc2.bias':                   'encoder.transformer.layers.9.feed_forward.output_dense.bias',
'encoder.layers.9.final_layer_norm.weight':    'encoder.transformer.layers.9.final_layer_norm.weight',
'encoder.layers.9.final_layer_norm.bias':      'encoder.transformer.layers.9.final_layer_norm.bias',

'encoder.layers.10.self_attn.k_proj.weight':    'encoder.transformer.layers.10.attention.k_proj.weight',
'encoder.layers.10.self_attn.k_proj.bias':      'encoder.transformer.layers.10.attention.k_proj.bias',
'encoder.layers.10.self_attn.v_proj.weight':    'encoder.transformer.layers.10.attention.v_proj.weight',
'encoder.layers.10.self_attn.v_proj.bias':      'encoder.transformer.layers.10.attention.v_proj.bias',
'encoder.layers.10.self_attn.q_proj.weight':    'encoder.transformer.layers.10.attention.q_proj.weight',
'encoder.layers.10.self_attn.q_proj.bias':      'encoder.transformer.layers.10.attention.q_proj.bias',
'encoder.layers.10.self_attn.out_proj.weight':  'encoder.transformer.layers.10.attention.out_proj.weight',
'encoder.layers.10.self_attn.out_proj.bias':    'encoder.transformer.layers.10.attention.out_proj.bias',
'encoder.layers.10.self_attn_layer_norm.weight':'encoder.transformer.layers.10.layer_norm.weight',
'encoder.layers.10.self_attn_layer_norm.bias':  'encoder.transformer.layers.10.layer_norm.bias',
'encoder.layers.10.fc1.weight':                 'encoder.transformer.layers.10.feed_forward.intermediate_dense.weight',
'encoder.layers.10.fc1.bias':                   'encoder.transformer.layers.10.feed_forward.intermediate_dense.bias',
'encoder.layers.10.fc2.weight':                 'encoder.transformer.layers.10.feed_forward.output_dense.weight',
'encoder.layers.10.fc2.bias':                   'encoder.transformer.layers.10.feed_forward.output_dense.bias',
'encoder.layers.10.final_layer_norm.weight':    'encoder.transformer.layers.10.final_layer_norm.weight',
'encoder.layers.10.final_layer_norm.bias':      'encoder.transformer.layers.10.final_layer_norm.bias',

'encoder.layers.11.self_attn.k_proj.weight':    'encoder.transformer.layers.11.attention.k_proj.weight',
'encoder.layers.11.self_attn.k_proj.bias':      'encoder.transformer.layers.11.attention.k_proj.bias',
'encoder.layers.11.self_attn.v_proj.weight':    'encoder.transformer.layers.11.attention.v_proj.weight',
'encoder.layers.11.self_attn.v_proj.bias':      'encoder.transformer.layers.11.attention.v_proj.bias',
'encoder.layers.11.self_attn.q_proj.weight':    'encoder.transformer.layers.11.attention.q_proj.weight',
'encoder.layers.11.self_attn.q_proj.bias':      'encoder.transformer.layers.11.attention.q_proj.bias',
'encoder.layers.11.self_attn.out_proj.weight':  'encoder.transformer.layers.11.attention.out_proj.weight',
'encoder.layers.11.self_attn.out_proj.bias':    'encoder.transformer.layers.11.attention.out_proj.bias',
'encoder.layers.11.self_attn_layer_norm.weight':'encoder.transformer.layers.11.layer_norm.weight',
'encoder.layers.11.self_attn_layer_norm.bias':  'encoder.transformer.layers.11.layer_norm.bias',
'encoder.layers.11.fc1.weight':                 'encoder.transformer.layers.11.feed_forward.intermediate_dense.weight',
'encoder.layers.11.fc1.bias':                   'encoder.transformer.layers.11.feed_forward.intermediate_dense.bias',
'encoder.layers.11.fc2.weight':                 'encoder.transformer.layers.11.feed_forward.output_dense.weight',
'encoder.layers.11.fc2.bias':                   'encoder.transformer.layers.11.feed_forward.output_dense.bias',
'encoder.layers.11.final_layer_norm.weight':    'encoder.transformer.layers.11.final_layer_norm.weight',
'encoder.layers.11.final_layer_norm.bias':      'encoder.transformer.layers.11.final_layer_norm.bias',

'encoder.layer_norm.weight': 'encoder.transformer.layer_norm.weight',
'encoder.layer_norm.bias': 'encoder.transformer.layer_norm.bias',

'layer_norm.weight': 'encoder.feature_projection.layer_norm.weight',
'layer_norm.bias': 'encoder.feature_projection.layer_norm.bias',
#'final_proj.weight': '',
#'final_proj.bias': '',
}

for key in weights:
    if key in conversion_weight_labels:
        new_weights.update([(conversion_weight_labels[key], weights[key])])

# new_weights = weights

# model['weight'] = new_weights
# model["config"][-3] = model["weight"]["emb_g.weight"].shape[0]

# Test that the import works
# from infer.lib.infer_pack.models import SynthesizerTrnMs768NSFsid
import torchaudio

hubert_model = torchaudio.models.hubert_base()

hubert_model.load_state_dict(new_weights)
torch.save(hubert_model.state_dict(), "assets/hubert/hubert_base_torch.pt")


print("hello")  # used for breakpoint